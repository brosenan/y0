(ns conditions)

;; # $y_0$ Rules and Conditions

;; As seen in the [introduction](hello.md), a $y_0$ program is composed of rules.

;; In the introduction we have covered trivial rules, rules of the form
;; `(all [vars...] head)`, where `vars...` are zero or more symbols representing
;; free (unbound) variables and `head` is a s-expression that is a pattern that
;; may contain the symbols in `vars...`. To recap, here is an example for a rule:
(all [x]
     (twice x [x x]))


;; This rule defines a predicate (`twice` with two arguments) that associates
;; anything with a vector containing that thing twice. For example:
(assert
 (twice 1 [1 1])
 (twice "two" ["two" "two"])
 (twice (x (y z)) [(x (y z)) (x (y z))]))

;; These examples show how `twice` duplicates different "things": numbers,
;; strings and arbitrary s-expressions.

;; While we can have a limited amount of fun with trivial rules, in order to
;; achieve our goal defining the semantics of languages through their ASTs,
;; we need to go deeper. In this case, we need to be able to go deeper in
;; the AST, to check properties of child nodes. trivial rules can only
;; match a single node. So to do more things we need to go beyond triviality,
;; and introduce _deduction rules_.

;; ## Deduction Rules

;; Deduction rules are rules that allow to deduce something based on one or
;; more conditions. It has the form:
;; ```clojure
;; (all [vars...] head <- conditions...)
;; ```
;; Where `vars...` are zero or more symbols that are used as free variables
;; in the rest of the rule, `head` is a goal pattern, like in trivial rules
;; and `conditions...` are zero or more _conditions_. We will discuss what
;; they can be, below.

;; ### Goal Conditions

;; The simplest type of condition is a _goal condition_, which is simply a
;; goal. A goal is an s-expression that matches the head of some rule,
;; so by using goal conditions we allow one rule to invoke other rules,
;; and even themselves (recursion), as we do in the following example.

;; Arguably, the simplest language with a recursive definition is the
;; [Peano numbers)(https://wiki.haskell.org/Peano_numbers). With words, one
;; would define them as follows:

;; 1. `z` is a Peano number.
;; 2. Given `n`, a Peano number, `(s x)` is also a peano number.

;; We can formulate this in $y_0$ as follows. First we define a base rule
;; that rejects everything, except things that will later be defined as
;; Peano numbers.
(all [x] (peano x ! x "is not a Peano number"))

;; Now we provide a trivial rule for the first part of the definition: `z`
;; is a Peano number.
(all [] (peano z))

;; And now we get to the recursive part:
(all [n]
     (peano (s n)) <- (peano n))

;; In this deduction rule, we deduce that `(s n)` is a Peano number for
;; every `n` that is a Peano number itself.

;; Now let us test this predicate.
(assert
 (peano z)
 (peano (s z))
 (peano (s (s (s (s z)))))
 (peano (s y) ! y "is not a Peano number")
 (peano (s (s (k (s z)))) ! (k (s z)) "is not a Peano number"))

;; The first three examples are positive examples, showing us that the
;; Peano equivalents of 0, 1 and 4 are indeed Peano numbers. The last
;; two example shows that anything that isn't a Peano number is rejected.

;; Please note that the rejection is done at the lowest level possible.
;; The error message does not state that `(s y)` is not a Peano number
;; but rather says this about `y`. This is because `(s x)` for some `x`
;; _could be_ a Peano number. The thing that definitely cannot be a
;; Peano number is `y` and therefore the error complaints about it.

;; This is similar to a compiler pointing the user to the exact line
;; or symbol in which the compilation error originated, rather than
;; just saying "your program is broken for some reason."

;; The ability to point out exactly where in the AST the problem
;; originates is what makes $y_0$ special, compared with other logic
;; programming languages (yes, $y_0$ is a logic-programming language).

;; In particular, this is why $y_0$ looks for rules that best match goals
;; rather than giving all possible results, which is what languages like
;; Prolog and Datalog would do. By choosing the best match in advance,
;; the program is committed to the chosen rule. And if something fails
;; down the line, we know there is no alternative. We know the AST is
;; invalid.

;; ## `exist` Conditions

;; So far we have introduced one way of introducing free variables into
;; a rule, following the `all` keyword. The `all` keyword imlies that
;; the variables are
;; [universally-quantified](https://en.wikipedia.org/wiki/Universal_quantification).
;; This allows the rule to apply for every assignment of every variable.

;; However, when a variable is introduced by a condition, it often
;; needs to be
;; [existentially-quantified](https://en.wikipedia.org/wiki/Existential_quantification).

;; To explin this, let us dive into an example. Let us consider a simple
;; expression language. In this language there are two terminals, `foo`
;; and `bar`, and two binary operators: `+` and `*`. An expression can
;; have either type (`foolish` or `barley`), based on the following rules:

;; 1. `foo` is `foolish`.
;; 2. `bar` is `barley`.
;; 3. Operators require both operands to be of the same type, but set the
;;    resulting type such that `+` is always `foolish` and `*` is always
;;    `barley`.

;; This can be expressed in $y_0$ using the following rules:
(all [x t] (foobar-type x t ! x "is not a foobar expression"))
(all [] (foobar-type foo foolish))
(all [] (foobar-type bar barley))
(all [a b] (foobar-type (+ a b) foolish) <-
     (exist [type-a type-b]
            (foobar-type a type-a)
            (foobar-type b type-b)
            (= type-a type-b ! "Type mismatch. Expression" a "is" type-a
               "but" b "is" type-b)))
(all [a b] (foobar-type (* a b) barley) <-
     (exist [type-a type-b]
            (foobar-type a type-a)
            (foobar-type b type-b)
            (= type-a type-b ! "Type mismatch. Expression" a "is" type-a
               "but" b "is" type-b)))

;; The first rule provides an explanation for all terms that are
;; structurally not in the language. The second and third rules are
;; trivial rules defining the case for `foo` and `bar`. The third and
;; fourth rule are the interesting ones. They define the type of the
;; `+` and `*` operators respectively. Because the type is internal
;; to the rule, it is introduced by an `exist` block, containing two
;; goal conditions, checking the type of the two operands. On the second
;; condition there is an explanation. This is mandatory since after
;; a successful application of the first condition, `type` is bound
;; to a concrete type (either `foolish` or `barley`). If the two
;; operands do not have the same type, this explanation will be
;; provided.
(assert
 (foobar-type foo foolish)
 (foobar-type bar barley)
 (foobar-type (+ foo foo) foolish)
 (foobar-type (+ bar bar) foolish)
 (foobar-type (* bar bar) barley)
 (foobar-type (* foo foo) barley)
 (foobar-type (+ foo bar) foolish
              ! "Type mismatch. Expression" foo "is" foolish "but" bar "is" barley))

;; Variables introduced in an `exist` condition override variables
;; of the same name introduced at the rule level (or outer `exist`
;; conditions). For example, let us introduce operator `++` which
;; is defined using a rule that is almost identical to that of
;; `+`, with only one exception. We add `a` to the list of
;; existentially-quantified variables.
(all [a b] (foobar-type (++ a b) foolish) <-
     (exist [type-a type-b a]
            (foobar-type a type-a)
            (foobar-type b type-b)
            (= type-a type-b ! "Type mismatch. Expression" a "is" type-a
               "but" b "is" type-b)))

;; This small difference changes everything.
(assert
 (foobar-type (++ foo foo) foolish ! _ "is not a foobar expression"))

;; Variable `a` introduced at the rule level, the one that is bound
;; to the first operand, is now overriden by a fresh,
;; existentially-quantified variable. This means that when trying to
;; figure-out `a`'s type, we call `foobar-type` on an unbound
;; variable, which makes us fall back to the base rule.

;; ## `given` Conditions

;; One important aspect of semantic analysis is the notion of scope.
;; Not everything that is true in one context is true in other
;; contexts. There are different ways in which scope can be captured.
;; One common method is to hold a data structure that propagates
;; through the analysis, containing information on things in scope.

;; However, in $y_0$, scopes are built into the language, in a
;; natural way, using `given` conditions.

;; A `given` condition is a condition of the form:
;; `(given statement conditions...)`, where `statement` is a statement
;; and `conditions...` are zero or more conditions.

;; The meaning of this condition is, given that we apply `statement`,
;; all of `conditions...` must be hold.

;; ### Example: The Lambda Calculus

;; To motivate and demonstrate the power and elegance of the
;; `given` condition, let us consider a predicate that accepts
;; expressions in the lambda calculus.

;; Syntactically, the lambda calculus is simple. An expression
;; can be one of three things:

;; 1. A symbol `var`, representing a variable,
;; 2. A lambda abstraction `(lambda var expr)`, where `var` is a
;;    symbols and `expr` is a lambda-calculus expression, and
;; 3. An application `(func arg)`, where both `func` and `arg`
;;    are lambda-calculus expressions.

;; Semanically, however, it is harder to know if a given expression
;; is valid. Variables cannot be used unless they are defined.
;; Constants can be defined using a `(defconst var expr)` statement,
;; or they can be introduced by a lambda abstraction, as the
;; parameter. The following definitions define the validity of a
;; lambda-calculus expression.

;; As usual, we begin with the base case, defining an error to be
;; provided for non-lambda expressions.
(all [x]
     (lambda-expr x ! x "is not a lambda-calulus expression"))
(assert
 (lambda-expr foo ! foo "is not a lambda-calulus expression"))

;; Next, let us define the `defconst` statement, which contributes
;; a symbol to `lambda-expr`.
(all [var expr]
     (defconst var expr) => (all [] (lambda-expr var)))

(defconst id (lambda x x))
(assert
 (lambda-expr id))

;; Now we define the rules for application, which is a simple
;; deduction rule with goal conditions.
(all [func arg]
     (lambda-expr (func arg)) <-
     (lambda-expr func)
     (lambda-expr arg))

(assert
 (lambda-expr (id id)))

;; This is a [translation rule](statements.md) which gives meaning
;; to `defconst` statements by translating them into a rule for
;; `lambda-expr`.

;; And now we come to the lambda expression. Here, the challenge
;; is making the parameter symbol valid within the body, but only
;; there. In other words, we would like to _temporarily_ add
;; a rule for `lambda-expr`, making the parameter symbol valid
;; when evaluating the body, but remove it just after.

;; This is where the `given` condition comes in.
(all [var expr]
     (lambda-expr (lambda var expr)) <-
     (given (all [] (lambda-expr var))
            (lambda-expr expr)))

(assert
 (lambda-expr (lambda x x))
 (lambda-expr (lambda x y) ! y "is not a lambda-calulus expression"))

;; So now we can check complex lambda expressions, such as the
;; y-combinator.
(assert
 (lambda-expr (lambda f ((lambda x (f (x x))) (lambda x (f (x x)))))))
